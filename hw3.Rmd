---
title: "hw3"
author: "min"
date: "2022-10-13"
output: github_document
---
# problem 1

## preparation
```{r}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)
data("instacart")
```

#### Introduce the data
```{r}
instacart = 
  instacart %>% 
  as_tibble(instacart)

instacart
```
*The data has 15 columns (variables) and 1384617 rows (observations). The variables starts with order_id, product_id for identification and variables like product_name, aisle, departments specify the goods. For example, product_id 49302 is Bulgarian Yogurt, which is in aisle yogurt department dairy eggs.*

In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` different products in `r instacart %>% select(order_id) %>% distinct %>% count` different orders from `r instacart %>% select(user_id) %>% distinct %>% count` different users.


#### answer the questions

count aisles
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
*There are 134 aisles, and fresh vegetables is where the most items ordered from*

plot of the number of items ordered in each aisle, only in which more than 10000 items ordered
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle,y = n)) + 
  geom_point() + 
  labs(title = "number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Table of the three most popular items in aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Table of the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered everyday
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow + 1, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

# problem 2

## tidy the data
```{r}
acc_data = read.csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  rename(day_of_the_week = day, day = day_id) %>% 
  pivot_longer(activity_1:activity_1440,
               names_to = "minute_from_midnight",
               names_prefix = "activity.",
               values_to = "activity_counts") %>% 
  mutate(minute_from_midnight = as.numeric(minute_from_midnight)) %>% 
  mutate(weekday_vs_weekend = 
           ifelse(day_of_the_week == c("Saturday","Sunday"),"weekend","weekday"))

acc_data
```


## aggregate table
```{r}
acc_data %>% 
  group_by(week, day_of_the_week) %>% 
  summarize(total = sum(activity_counts)) %>% 
  pivot_wider(names_from = day_of_the_week,
              values_from = total) %>% 
  select(week, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday)

acc_data %>% 
  group_by(day, minute_from_midnight,weekday_vs_weekend) %>% 
  summarize(total = sum(activity_counts)) %>% 
  arrange(desc(total)) %>% 
  head(20)
```
*There is no apparent trend in the first table, but in the second, we can see it tends to have higher activity counts in weekdays, and it is more likely to have higher activity counts at around 1170 minutes from midnight.*

## plot
```{r}
acc_data %>% 
  ggplot(aes(x = minute_from_midnight,y = activity_counts, color = day_of_the_week)) +
  geom_point(alpha = .1) +
  geom_smooth(se = FALSE) +
  ylim(NA,2500)
```
*The activity counts are low and steady in the first place, then it starts to rise at minute 250, achieve a climax around minute 600, decreases, rises again, achieves another climax around 1100, especially in Friday and Sunday.*

# problem 3

```{r}
data("ny_noaa")
str(ny_noaa)
ny_noaa %>% drop_na()
```
*There are 7 variables and 2595176 observations in this data, only 1222433 observations contain no missing values. The dataset identify each observation with id and date, and then describes prcp,snow,snwd and temperature min&max in each observation.*

## tidy the data
```{r}
tidy_ny = ny_noaa %>% 
  separate(col = date, into = c("year","month","day")) %>% 
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day)) %>%
  mutate(tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10, prcp = prcp/10)

str(tidy_ny)
```
*the most commonly observed values for snowfall is `r tidy_ny %>% count(snow) %>% arrange(desc(n)) %>% head(1) %>% select(snow)`, as normally, there is no snow.*

## plot
```{r}
tidy_ny %>% 
  filter(month == c(1,7), !is.na(tmax)) %>% 
  group_by(id,year,month) %>% 
  summarize(ave_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = ave_tmax)) + 
  geom_point() + 
  facet_grid(.~month)

```
*The ave_tmax in July is significantly higher than ave_tmax in January. Outliers exist*

## tmax vs tmin
```{r}
tidy_ny %>% 
  drop_na(c(tmax,tmin)) %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex()
```

## snowfall distribution
```{r}
tidy_ny %>% 
  filter(snow > 0,snow < 100) %>% 
  ggplot(aes(x = snow, y = factor(year))) + 
  geom_density_ridges(scale = .85)
```

